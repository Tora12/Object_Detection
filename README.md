# Object Detection Brief Report
## SIFT – Scale Invariant Feature Transform

The four main steps for the SIFT algorithm are:
1) Scale-space peak selection
2) Key point localization
3) Orientation Assignment
4) Key point descriptor

 Scale space is what allows SIFT to be invariant to image scaling. Roughly speaking, the scale space of an image is a function that is produced from the convolution of a Gaussian kernel (i.e. blurring) at different scales of the input image. The scale space is separated into octaves, where each octave is roughly half the size of the precious one. Within each octave, the input image is progressively blurred and we use these blurred images to generate another set of images called the difference of Gaussians for all the different octaves of the image in the Gaussian Pyramid. We can use the different of Gaussian images to find interesting key points and to calculate Laplacian of Gaussian approximations which are scale invariant. This however produces a lot of key points and therefore the next step is to remove the features that are not useful. For low contrast features we simply check if the intensity at the extrema is greater than the threshold value and if not reject it. After we filter through and have the “good” key points, we assign an orientation to each of the key points making it rotation invariant. This is done using an orientation histogram, and we take the highest peak along with any that are above a certain percent threshold. Since each key point now has a location, scale and an orientation, next is to compute a descriptor for the local image region for each keypoint. Lastly we just match the keypoints between the two images which are matched by identifying their nearest neighbors.
 
 Overall, the SIFT algorithm is able to find the highest number of key points within an image and also the highest number of matching key points between two images. Although, it still picks up quite a bit of noise in the overall image, compared to BRIEF & FAST, SIFT still does a pretty good job matching key points on the object between two different images.
 
## BRIEF & FAST – Binary Robust Independent Element Features & Features from Accelerated Segment Test

 FAST is a corner detection method which can be used to extract feature points and later used to track, map and or match objects. How the FAST algorithm works is first we select a pixel p in the image to determine whether it is an interest point or not. Then we select an appropriate threshold value t. Next we consider a circle of 16 pixels around our pixel of interest and if there exists a set of n contiguous pixels in the circle of 16 pixels which are all brighter than the pixel intensity p + threshold value t or all darker than the pixel intensity p – threshold value t, then we can consider pixel p a corner. The high speed that we get from using the FAST algorithm is from looking at certain pixels first before deciding whether to apply the full algorithm.
 
 BRIEF on the other hand cannot find feature but can find location pairs and compare pixel intensities. Therefore in my code I use the FAST algorithm to find the feature points and use those points to compute a descriptor for every one of the feature points. BRIEF deals with the image at the pixel level meaning it is very sensitive to noise within the image (as we can see below).
 
 BRIEF & FAST had the lowest numbers in detecting key points and picked up mostly on the noise in the image with very few key points actually being on the object itself. We could improve our results a few ways. First, we could reduce the noise in our image by pre-smoothing our image more before utilizing the BRIEF algorithm. Second, we could use the STAR method in OpenCV which is what the BRIEF paper recommends. Thirdly, if we know roughly where the object of interest will be in the image then we could focus in on that part of the image to get our key points thus reducing the potential of picking up noise as key points within the image.
 
## ORB – Oriented FAST and Rotated BRIEF
 ORB builds on the FAST key point detector and the BRIEF descriptor algorithms. Both these techniques/algorithms are desirable to us because of their fast and good performance plus a low computing cost. We first use the FAST algorithm (described above) to determine edges and find key points. However, the features found by FAST do not have an orientation component or multi- scale features. Thus, the ORB algorithm uses an image pyramid for multi-scale. Once we create this multi-scale image pyramid we apply the FAST algorithm to each of the levels in the pyramid to locate key points at different scales. To get the orientation we detect intensity change using an intensity centroid and assumes that an intensity of a corner is. The intensity centroid assumes that an intensity of a corner is offset from its center, and we use this vector to represent the orientation. Rather than using BRIEF which isn’t invariant to rotation, ORB uses rBRIEF (rotation-aware BRIEF) without losing out on the high speed that we desire from using BRIEF. Whereas BRIEFs matching performance of BRIEF falls off for in-plane rotations of more than a few degrees, rBRIEF uses a method to steer BRIEF to the orientation of the key points. Overall, rBRIEF shows significant improvement in variance and correlation over BRIEF.
 
 Although the number of key points found is much lower than the SIFT algorithm, ORB does a much better just at finding and matching key points on the object from one image to another image. There was also noticeable difference in speed comparing SIFT and ORB as ORB ran faster and was comparable to BRIEF & FAST while performing just as well if not better than SIFT.
 
A couple of things that may have effected the overall performance of all of the algorithms are:
1) I took fairly detailed photos which could have contributed to part of the problem of picking up noise and also detecting not so useful feature points. As I stated before we might be able
to improve results by improving the smoothing/blurring process of our image to reduce the chance of noise in the overall image. We could likely also describe an area of interest in the image so that we can focus more on detecting the object rather than noise in the surrounding environment.
2) This might be a stretch but my Snoopy stuffed animal might not be the easiest object to detect as both SIFT, BRIEF and FAST picked just as much if not more key points from noise or from the background rather than our object of interest itself.
